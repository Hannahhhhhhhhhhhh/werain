{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in d:\\anaconda\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: Cython==0.29.21 in d:\\anaconda\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Requirement already satisfied: numpy>=1.11.3 in d:\\anaconda\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\anaconda\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\anaconda\\lib\\site-packages (from gensim) (5.1.0)\n",
      "Collecting jieba\n",
      "  Downloading https://files.pythonhosted.org/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2MB)\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Hannah\\AppData\\Local\\pip\\Cache\\wheels\\af\\e4\\8e\\5fdd61a6b45032936b8f9ae2044ab33e61577950ce8e0dec29\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim\n",
    "! pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.analyse  \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 94043 entries, 0 to 94042\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   length_text          94043 non-null  int64 \n",
      " 1   length_retweet_text  94043 non-null  int64 \n",
      " 2   text_pro             94043 non-null  object\n",
      " 3   retweet_text_pro     41922 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "all_text = pd.read_csv('all_text.csv')\n",
    "all_text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_text</th>\n",
       "      <th>length_retweet_text</th>\n",
       "      <th>text_pro</th>\n",
       "      <th>retweet_text_pro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>这是不是无耻到一定境界了？抗击暴雨的普通百姓，居然是北京精神的作用？人们的团结热心，居然是某...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>142</td>\n",
       "      <td>对比北京暴雨</td>\n",
       "      <td>【俄罗斯洪灾 3名官员因失职被捕】俄罗斯南部于今年7月7日，遭暴雨袭击并引发洪灾，亡人数达1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>北京市民坚持住!众志成城,定可度过暴雨阶段!!!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>北京暴雨死了这么多人，不但新闻上不了头条，连主持人都穿着这么鲜艳的衬衣。真是一个毫无人性的国家！</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>124</td>\n",
       "      <td>大眼，好样的！从什邡到北京暴雨，那些公知们忙着晒裸照，约口水战粉饰太平时，很高兴看到还有你这...</td>\n",
       "      <td>山后再铁《图腾》——“这几乎是个图腾，只有上善若水，没有从善如流，不是城市不足，而是成事不足...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length_text  length_retweet_text  \\\n",
       "0           79                    0   \n",
       "1            6                  142   \n",
       "2           24                    0   \n",
       "3           48                    0   \n",
       "4          102                  124   \n",
       "\n",
       "                                            text_pro  \\\n",
       "0  这是不是无耻到一定境界了？抗击暴雨的普通百姓，居然是北京精神的作用？人们的团结热心，居然是某...   \n",
       "1                                             对比北京暴雨   \n",
       "2                           北京市民坚持住!众志成城,定可度过暴雨阶段!!!   \n",
       "3   北京暴雨死了这么多人，不但新闻上不了头条，连主持人都穿着这么鲜艳的衬衣。真是一个毫无人性的国家！   \n",
       "4  大眼，好样的！从什邡到北京暴雨，那些公知们忙着晒裸照，约口水战粉饰太平时，很高兴看到还有你这...   \n",
       "\n",
       "                                    retweet_text_pro  \n",
       "0                                                NaN  \n",
       "1  【俄罗斯洪灾 3名官员因失职被捕】俄罗斯南部于今年7月7日，遭暴雨袭击并引发洪灾，亡人数达1...  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4  山后再铁《图腾》——“这几乎是个图腾，只有上善若水，没有从善如流，不是城市不足，而是成事不足...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "筛选原创文本，即转发文本长度length_retweet_text为0的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_text所有文本的数目为： 94043\n",
      "原创微博文本的数目为： 47390\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_text</th>\n",
       "      <th>text_pro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>这是不是无耻到一定境界了？抗击暴雨的普通百姓，居然是北京精神的作用？人们的团结热心，居然是某...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>北京市民坚持住!众志成城,定可度过暴雨阶段!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>北京暴雨死了这么多人，不但新闻上不了头条，连主持人都穿着这么鲜艳的衬衣。真是一个毫无人性的国家！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>@中国泛户外第一人李辉 @梁强户外休闲观察 户外休闲产业发展和户外灾害生存教育应该同步进行、...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139</td>\n",
       "      <td>截至2012年7月22日17时，北京暴雨后共发现37人死亡。1949年以来，全国的人民和财力...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47385</th>\n",
       "      <td>22</td>\n",
       "      <td>@下一秒_独自等待    亲，北京的雨大不？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47386</th>\n",
       "      <td>66</td>\n",
       "      <td>北京气象部门已成惊弓之鸟 每天工作第一件事就是预报有雨 管它下不下 先让大伙不出门就行 我在...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47387</th>\n",
       "      <td>74</td>\n",
       "      <td>哥回北京，果然天气放晴，哎这孩子就是不给雨… 从机场回来一路绿灯，连闯红灯的机会都不给，太没...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47388</th>\n",
       "      <td>84</td>\n",
       "      <td>雨一直下，气氛不算融洽...坝上疯行下回吧，据说城里都晴了，可是北面还是雨ing。找地吃鱼，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47389</th>\n",
       "      <td>130</td>\n",
       "      <td>面对隐患当未雨绸缪 | 据哈尔滨天气预报，未来的几天我市还有中到大雨和雷阵雨。北京等地的大雨...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47390 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       length_text                                           text_pro\n",
       "0               79  这是不是无耻到一定境界了？抗击暴雨的普通百姓，居然是北京精神的作用？人们的团结热心，居然是某...\n",
       "1               24                           北京市民坚持住!众志成城,定可度过暴雨阶段!!!\n",
       "2               48   北京暴雨死了这么多人，不但新闻上不了头条，连主持人都穿着这么鲜艳的衬衣。真是一个毫无人性的国家！\n",
       "3              120  @中国泛户外第一人李辉 @梁强户外休闲观察 户外休闲产业发展和户外灾害生存教育应该同步进行、...\n",
       "4              139  截至2012年7月22日17时，北京暴雨后共发现37人死亡。1949年以来，全国的人民和财力...\n",
       "...            ...                                                ...\n",
       "47385           22                             @下一秒_独自等待    亲，北京的雨大不？\n",
       "47386           66  北京气象部门已成惊弓之鸟 每天工作第一件事就是预报有雨 管它下不下 先让大伙不出门就行 我在...\n",
       "47387           74  哥回北京，果然天气放晴，哎这孩子就是不给雨… 从机场回来一路绿灯，连闯红灯的机会都不给，太没...\n",
       "47388           84  雨一直下，气氛不算融洽...坝上疯行下回吧，据说城里都晴了，可是北面还是雨ing。找地吃鱼，...\n",
       "47389          130  面对隐患当未雨绸缪 | 据哈尔滨天气预报，未来的几天我市还有中到大雨和雷阵雨。北京等地的大雨...\n",
       "\n",
       "[47390 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先处理原创微博文本\n",
    "print('all_text所有文本的数目为：', len(all_text))\n",
    "df_text = all_text[all_text['length_retweet_text']==0][['length_text','text_pro']].reset_index(drop=True)\n",
    "df_text.drop_duplicates(subset='text_pro',inplace=True)\n",
    "df_text = df_text.reset_index(drop=True)\n",
    "print('原创微博文本的数目为：', len(df_text))\n",
    "\n",
    "text = df_text['text_pro']\n",
    "df_text\n",
    "\n",
    "# pattern_text = r'// 分享网易新闻:*|// 分享网易新闻:「*|//分享网易*'\n",
    "# text = [re.sub(pattern_text,' ',df_text['text_pro'][i]) for i in range(len(df_text['text_pro']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评论数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clear(all_comments,pattern_special = r'//@.*:', pattern_text = r'分享网易新闻'):\n",
    "    \n",
    "    # 1. 删除由特殊格式限定的广告、用户名、链接等\n",
    "    x1 = all_comments\n",
    "    x2 = re.sub(pattern_special,\" \",x1)\n",
    "    \n",
    "    # 2. 仅保留中文\n",
    "    pattern = re.compile(r'[\\u4e00-\\u9fa5]+')\n",
    "    filedata = re.findall(pattern, x2)\n",
    "    x3 = ''.join(filedata)\n",
    "    \n",
    "    # 3. 去掉微博评论特有的无效信息\n",
    "    new_xx = re.sub(pattern_text,'',x3)\n",
    "    return new_xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_special = r'//@.*:|//@.*：|(来自.*)|@.*\\s|http://.*'\n",
    "pattern_text = r'分享网易新闻|分享|转载|微博调查|我发起了一个投票'\n",
    "new_text = [ data_clear(text[i],pattern_special,pattern_text) for i in range(len(text))]\n",
    "\n",
    "df_new_text = pd.DataFrame(data = [len(new_text[j]) for j in range(len(new_text))],columns=['length_text'])\n",
    "df_new_text['new_text'] = new_text\n",
    "# df_new_text.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除字符数小于15的短文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除短文本前评论数目： 47390\n",
      "删除短文本后评论数目： 41642\n"
     ]
    }
   ],
   "source": [
    "print(\"删除短文本前评论数目：\",len(df_new_text['length_text']))\n",
    "new_text = df_new_text[df_new_text['length_text']>=15]['new_text'].reset_index(drop=True)\n",
    "print('删除短文本后评论数目：',len(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选原创文本\n",
    "# 评论数据清洗\n",
    "# 删除短文本\n",
    "new_text.to_csv('original_text.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主题抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词、去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_text = pd.read_csv('original_text.csv')\n",
    "new_text = new_text['new_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_stop(file_stop):\n",
    "    stopwords = [line.strip() for line in open(file_stop, 'r', encoding='GBK').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "    #利用jieba进行分词\n",
    "def seg_sentence(sentence):\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    stopwords = open_stop(\"chineseStopWords.txt\")  \n",
    "    outstr = ''\n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是不是无耻到一定境界了抗击暴雨的普通百姓居然是北京精神的作用人们的团结热心居然是某几位领导干部官员定下的北京精神的号召结果新华网当真无耻\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Hannah\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.788 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无耻 境界 抗击 暴雨 普通百姓 居然 北京 精神 作用 团结 热心 居然 几位 领导 干部 官员 定下 北京 精神 号召 新华网 当真 无耻\n"
     ]
    }
   ],
   "source": [
    "# 分词示例\n",
    "sentence = new_text[0]\n",
    "print(sentence)\n",
    "print(seg_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 41642/41642 [00:48<00:00, 862.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 41642/41642 [00:00<00:00, 174914.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# 批量分词 \n",
    "words_in_sentence = []\n",
    "for i in tqdm(range(len(new_text))):\n",
    "    line_seg = seg_sentence(new_text[i])  # 这里的返回值是字符串\n",
    "    words_in_sentence.append(line_seg)\n",
    "words_in_sentence = [x for x in words_in_sentence if x != '']\n",
    "\n",
    "#利用空格切割获取所有分词\n",
    "alltokens = []\n",
    "chinesewords_sentence = []\n",
    "for i in tqdm(range(len(words_in_sentence))):\n",
    "    word = re.split(r'\\s',words_in_sentence[i])\n",
    "    alltokens.append(word)\n",
    "\n",
    "#对每一个句子产生的分词所储存的列表，删除空值\n",
    "for element in alltokens:\n",
    "    element = [x for x in element if x != '']\n",
    "    chinesewords_sentence.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是不是无耻到一定境界了抗击暴雨的普通百姓居然是北京精神的作用人们的团结热心居然是某几位领导干部官员定下的北京精神的号召结果新华网当真无耻\n",
      "\n",
      "无耻 境界 抗击 暴雨 普通百姓 居然 北京 精神 作用 团结 热心 居然 几位 领导 干部 官员 定下 北京 精神 号召 新华网 当真 无耻\n",
      "\n",
      "['无耻', '境界', '抗击', '暴雨', '普通百姓', '居然', '北京', '精神', '作用', '团结', '热心', '居然', '几位', '领导', '干部', '官员', '定下', '北京', '精神', '号召', '新华网', '当真', '无耻']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(new_text[0],end='\\n\\n')\n",
    "print(words_in_sentence[0],end='\\n\\n')\n",
    "print(chinesewords_sentence[0],end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_text = pd.DataFrame(new_text,columns=['new_text'])\n",
    "df_new_text.head()\n",
    "df_new_text.to_csv('df_new_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of comments:  1668872\n"
     ]
    }
   ],
   "source": [
    "all_comments = \"\"\n",
    "for i in range(len(chinesewords_sentence)):\n",
    "    all_comments +=  ''.join(chinesewords_sentence[i]) + ' '\n",
    "print(\"Length of comments: \",len(all_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords=jieba.analyse.extract_tags(all_comments, topK=100, withWeight=True, allowPOS=('n','ns','nr'))\n",
    "word_tf_idf = {x[0]:x[1] for x in keywords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf = pd.DataFrame({'Keyword':list(word_tf_idf.keys()), 'TF_IDF':list(word_tf_idf.values())})\n",
    "df_tf_idf.to_csv('df_tf_idf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>TF_IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>北京</td>\n",
       "      <td>0.670162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>暴雨</td>\n",
       "      <td>0.504059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>大雨</td>\n",
       "      <td>0.338362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>积水</td>\n",
       "      <td>0.128988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>遇难者</td>\n",
       "      <td>0.071203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Keyword    TF_IDF\n",
       "0      北京  0.670162\n",
       "1      暴雨  0.504059\n",
       "2      大雨  0.338362\n",
       "3      积水  0.128988\n",
       "4     遇难者  0.071203"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_idf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sgns.baidubaike.bigram-char'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-5dd0430266dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sgns.baidubaike.bigram-char'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 再加载第三方预训练模型：\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1630\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1631\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1632\u001b[1;33m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1633\u001b[0m         )\n\u001b[0;32m   1634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1891\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1892\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1893\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1894\u001b[0m             \u001b[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sgns.baidubaike.bigram-char'"
     ]
    }
   ],
   "source": [
    "file_path = 'sgns.baidubaike.bigram-char'\n",
    "# 再加载第三方预训练模型：\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1066.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雨灾\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_tfidf_word = list(word_tf_idf.keys())\n",
    "vec_list = []\n",
    "word_list = []\n",
    "num = 0\n",
    "for word in tqdm(list_tfidf_word):\n",
    "    if word in model.index_to_key:\n",
    "        vec = model[word]\n",
    "        vec_list.append(vec)\n",
    "        word_list.append(word)\n",
    "    else:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_number = 7\n",
    "clf = KMeans(n_clusters=class_number)\n",
    "clf.fit(np.array(vec_list))  # 分组\n",
    "labels = clf.labels_ \n",
    "use_labels = list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 类：\n",
      "积水 雨量 下水道 雨水 防雨 \n",
      "\n",
      "第 2 类：\n",
      "城市 蜡烛 车门 头枕 路段 插头 玻璃 座位 雨伞 车主 车辆 动车 私家车 公司 汽车 信号 机场 童鞋 地铁 雨具 \n",
      "\n",
      "第 3 类：\n",
      "遇难者 悼念 大家 朋友 回家 蓝色 地区 逝者 新闻 老板 小时 中雨 时候 遭遇 政府 鼻屎 人民 视频 地方 奥特曼 时分 博文 人们 本市 亲们 市民 常识 群众 微风 领导 爱心 精神 问题 员工 浮云 丁先生 晚安 网友 感觉 小心 \n",
      "\n",
      "第 4 类：\n",
      "短信 电话 \n",
      "\n",
      "第 5 类：\n",
      "暴雨 大雨 降雨 强降雨 台风 天气 被淹 灾难 山洪 灾害 天灾 泥石流 洪水 暴风雨 \n",
      "\n",
      "第 6 类：\n",
      "气象台 天气预报 气象局 \n",
      "\n",
      "第 7 类：\n",
      "北京 北京市 房山 广渠门 天津 积水潭 平谷 通州 密云 中国 深圳 故宫 房山区 北京城 团城 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_list=[]\n",
    "for i in range(class_number):\n",
    "    class_list=[]\n",
    "    for j in range(len(labels)):\n",
    "        if clf.labels_[j]==i:\n",
    "            class_list.append(word_list[j])\n",
    "    all_list.append(class_list)\n",
    "\n",
    "for i in range(class_number):\n",
    "    print('第',i+1,'类：')\n",
    "    str_=''\n",
    "    for word in all_list[i]:\n",
    "        str_+=word\n",
    "        str_+=' '\n",
    "    print(str_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
